<!DOCTYPE HTML>
<html class="hb-loaded">

<head>
    <meta charset="UTF-8">
    <title>LVVU 2020</title>

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
</head>

<body huaban_collector_injected="true">

    <section class="page-header" style="width:900px;margin: auto; background-color:white">

        <h1 class="project-name">LVVU 2020</h1>
        <h4 class="project-tagline">Workshop on Language &amp; Vision with applications to Video Understanding<div
                style="float: right; clear: right;"><b>Date:</b> June 19, 2020, PDT time</div>
        </h4>
        <h4 class="project-tagline">In conjunction with <a href="http://cvpr2020.thecvf.com/">CVPR 2020</a>
            <div style="float: right; clear: right;"><b>Room:</b> Online</div>
        </h4>
        <!-- <br>&nbsp;&nbsp;&nbsp;&nbsp; -->


    </section>

    <section style="width:900px;margin: auto;  background-color:white">
        <!-- Static navbar -->
        <nav class="navbar navbar-default">
            <div class="container-fluid">
                <div class="navbar-header">
                    <butfton type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar"
                        aria-expanded="false" aria-controls="navbar">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>


                    </butfton>
                </div>
                <div id="navbar" class="navbar-collapse collapse">
                    <ul class="nav navbar-nav">
                        <li class="active"><a href="#invitedSpeakers">Speakers</a></li>
                        <li><a href="#schedule">Schedule</a></li>
                        <li><a href="https://languageandvision.github.io/ws/index.html">Papers</a></li>
                        <li><a href="#challenges">Challenges</a></li>
                        <li><a href="#overview">Overview &amp; CFP</a></li>
                        <!-- <li><a href="#submission-info">Submission</a></li> -->
                        <li><a href="#important-dates">Important Dates</a></li>
                        <li><a href="#organizers">Organizers</a></li>
                        <!-- <li><a href="#pastworkshops">Past Workshops</a></li> -->
                    </ul>
                </div>
                <!--/.nav-collapse -->
            </div>
            <!--/.container-fluid -->
        </nav>

    </section>

    <section class="main-content" style="width:900px;margin: auto;  background-color:white">


        <a id="invitedSpeakers" class="anchor" href="#invitedSpeakers" aria-hidden="true"><span
                class="octicon octicon-link"></span></a>
        <h2>Invited Speakers</h2>

        <font size="3">

            <ul>
                <li><a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>, Johns Hopkins University</li>
                <li><a href="https://taomei.me/">Tao Mei</a>, JD Research</li>
                <li><a href="http://jin-qin.com/">Qin Jin</a>, Renmin University of China</li>
                <li><a href="https://feichtenhofer.github.io/">Christoph Feichtenhofer</a>, Facebook AI Research (FAIR)
                </li>
            </ul>
        </font>

        <a id="schedule" class="anchor" href="#schedule" aria-hidden="true"><span
                class="octicon octicon-link"></span></a>

        <a id="schedule" class="anchor" href="#schedule" aria-hidden="true"><span
                class="octicon octicon-link"></span></a>
        <h2>Schedule [<a href="http://cvpr20.com/language-vision-for-video-understanding/">Live Session Landing
                Page</a>]</h2>
        <table class="table" style="width:100%">
            <tbody>
                <tr class="info">
                    <td>9:00-9:10</td>
                    <td><strong>Opening Remarks [Live]</strong></td>
                    <td><strong>Workshop Organizers</strong></td>
                </tr>

                <tr class="success">
                    <td>9:10-9:50</td>
                    <td><strong>Invited Talk 1 + Live Q&amp;A
                            <br> <a href="https://youtu.be/iiXeIfnISy4"> Compositional Models for Open-Set Activity
                                Classification of Videos </a> </strong></td>
                    <td><strong> Alan Yuille </strong></td>
                </tr>

                <tr class="success">
                    <td>9:50-10:30</td>
                    <td><strong>Invited Talk 2 + Live Q&amp;A
                            <br><a href="https://youtu.be/3Kya9e13fNE">Efficient Video Recognition</a></strong></td>
                    <td><strong>Christoph Feichtenhofer</strong></td>
                </tr>

                <tr>
                    <td>10:30-10:40</td>
                    <td><strong>Coffee Break</strong></td>
                    <td> </td>
                </tr>

                <tr class="success">
                    <td>10:40-11:20</td>
                    <td><strong> <a href="https://languageandvision.github.io/ws/index.html"> Invited Short Oral
                                (workshop paper session) </a></strong></td>
                    <td></td>
                </tr>

                <tr>
                    <td>11:20-14:00</td>
                    <td><strong>Lunch</strong></td>
                    <td> </td>
                </tr>

                <tr class="success">
                    <td>14:00-14:40</td>
                    <td><strong>Invited Talk 3 + Live Q&amp;A
                            <br><a href="https://youtu.be/478i73z1GAA">Image and Video Captioning</a></strong></td>
                    <td><strong>Qin Jin</strong></td>

                </tr>

                <tr class="success">
                    <td>14:40-15:10</td>
                    <td><strong>Invited Talk 4
                            <br><a href="https://youtu.be/UuO1N-HFlew">Vision and Language: From Perception to
                                Creation</a></strong></td>
                    <td><strong>Tao Mei</strong></td>
                </tr>

                <tr>
                    <td>15:10-15:30</td>
                    <td><strong>Coffee Break</strong></td>
                    <td> </td>
                </tr>

                <tr class="success">
                    <td>15:30-15:40</td>
                    <td><strong><a href="https://youtu.be/q6lZgwAslRs">YouMakeup Challenge</a></strong></td>
                    <td><strong>Shizhe Chen <a
                                href="https://drive.google.com/file/d/1b8YZq4Enek7jYZcJXxhGU4OqF49joAy_/view?usp=sharing">[Slides]</a></strong>
                    </td>
                </tr>

                <tr class="success">
                    <td>15:40-16:00</td>
                    <td><strong>Challenge Talk + Live Q&amp;A
                            <br><a href="https://youtu.be/gJz8JgiC0vo">Baseline Introduction and Analysis</a></strong>
                    </td>
                    <td><strong> Ludan Ruan, Linli Yao,<br> Weiying Wang</strong></td>
                </tr>

                <tr class="success">
                    <td>16:00-16:20</td>
                    <td><strong>Challenge Talk (winner)
                            <br><a href="https://youtu.be/oRCGmWopV-A">Multi-modal Feature Fusion for YouMakeup Video
                                Question Answering</a></strong>
                        <strong><br><a href="https://youtu.be/oRCGmWopV-A?t=14m21s">BUPTMM Submission for YouMakeUp VQA
                                Challenge in Step Ordering Task</a></strong>
                    </td>
                    <td><br><strong>Yajie Zhang, Li Su <a
                                href="https://languageandvision.github.io/ws/files/youmakeup_acdart.pdf">[report]</a>
                            <br>Kun Liu, Huadong Ma <a
                                href="https://languageandvision.github.io/ws/files/youmakeup_buptmm.pdf">[report]</a></strong>
                    </td>
                </tr>

                <tr class="success">
                    <td>16:20-16:30</td>
                    <td><strong>VATEX Captioning Challenge [Live]</strong></td>
                    <td><strong>Xin Wang</strong></td>
                </tr>

                <tr class="success">
                    <td>16:30-16:50</td>
                    <td><strong>Challenge Talk (winner) + Live Q&amp;A
                            <br><a href="https://youtu.be/dzjwQIROhcg">Multi-View Features and Hybrid Reward Strategies
                                for Video Captioning</a></strong></td>
                    <td><strong>Xinxin Zhu, Longteng Guo, Peng Yao, <br>Shichen Lu, Wei Liu, Jing Liu</strong></td>
                </tr>

                <tr class="success">
                    <td>16:50-17:10</td>
                    <td><strong>Challenge Talk (runner-up) + Live Q&amp;A
                            <br><a href="https://youtu.be/jDZBek1kMeU">Multi-modal Feature Fusion with Feature Attention
                                for Video Captioning</a></strong></td>
                    <td><strong>Ke Lin, Zhuoxin Gan, Liwei Wang</strong></td>
                </tr>

                <tr class="info">
                    <td>17:10-17:15</td>
                    <td><strong>Ending [Live]</strong></td>
                    <td><strong>Workshop Organizers</strong></td>
                </tr>



            </tbody>
        </table>

        <!-- <font size=3>
<ul>
<li><a href="https://aclweb.org/anthology/papers/W/W19/W19-1607/">SpatialNet: A Declarative Resource for Spatial Relations</a> <b>(Best Paper)</b><br><i><font size=2>Morgan Ulinski, Bob Coyne and Julia Hirschberg</font></i></li>
<li><a href="https://aclweb.org/anthology/papers/W/W19/W19-1605/">Multi-modal Discriminative Model for Vision-and-Language Navigation </a><b>(Best Paper)</b><br><i><font size=2>Haoshuo Huang, Vihan Jain, Harsh Mehta, Jason Baldridge and Eugene Ie </font></i></li>
<li><a href="https://aclweb.org/anthology/papers/W/W19/W19-1601/">Corpus of Multimodal Interaction for collaborative planning</a><br><i><font size=2>Miltiadis Marios Katsakioris, Helen Hastie, Ioannis Konstas and Atanas Laskov </font></i></li>
<li><a href="https://aclweb.org/anthology/papers/W/W19/W19-1602/">¿Es un plátano? Exploring the Application of a Physically Grounded Language Acquisition System to Spanish</a><br><i><font size=2>Caroline Kery, Cynthia Matuszek and Francis Ferraro</font></i></li>
<li><a href="https://aclweb.org/anthology/papers/W/W19/W19-1604/">Learning from Implicit Information in Natural Language Instructions for Robotic Manipulations</a><br><i><font size=2>Ozan Arkan Can, Pedro Zuidberg Dos Martires, Andreas Persson, Julian Gaal, Amy Loutfi, Luc De Raedt, Deniz Yuret and Alessandro Saffiotti </font></i></li>
<li><a href="https://aclweb.org/anthology/papers/W/W19/W19-1606/">Semantic Spatial Representation: a unique representation of an environment based on an ontology for robotic applications</a><br><i><font size=2>Guillaume Sarthou, Aurélie Clodic and Rachid Alami</font></i></li>
<li><a href="https://aclweb.org/anthology/papers/W/W19/W19-1603/">From Virtual to Real: A Framework for Verbal Interaction with Robots</a><br><i><font size=2>Eugene Joseph</font></i></li>
<li><a href="https://aclweb.org/anthology/papers/W/W19/W19-1608/">What a neural language model tells us about spatial relations</a><br><i><font size=2>Mehdi Ghanimifard and Simon Dobnik</font></i></li>
</ul>
</font>

<br><font size=5><b>Cross Submissions</b></font>
<font size=3>
<ul>
<li><a href="nonarchival/pillai.pdf" target="_blank">Deep Learning for Category-Free Grounded Language Acquisition</a><br><i><font size=2>Nisha Pillai, Francis Ferraro and Cynthia Matuszek</font></i></li>
<li><a href="https://arxiv.org/abs/1905.04655" target="_blank">Improving Natural Language Interaction with Robots Using Advice</a><br><i><font size=2>Nikhil Mehta and Dan Goldwasser</font></i></li>
<li><a href="https://arxiv.org/abs/1904.01650" target="_blank">Improving Robot Success Detection using Static Object Data</a><br><i><font size=2>Rosario Scalise, Jesse Thomason, Yonatan Bisk and Siddhartha Srinivasa</font></i></li>
<li><a href="https://arxiv.org/abs/1904.07165" target="_blank">Learning to Generate Unambiguous Spatial Referring Expressions for Real-World Environments</a><br><i><font size=2>Fethiye Irmak Dogan, Sinan Kalkan and Iolanda Leite</font></i></li>
<li>Learning to Ground Language to Temporal Logical Form<br><i><font size=2>Roma Patel, Stefanie Tellex and Ellie Pavlick</font></i></li>
<li><a href="https://arxiv.org/abs/1904.04195" target="_blank">Learning to Navigate Unseen Environments: Back Translation with Environmental Dropout</a><br><i><font size=2>Hao Tan, Licheng Yu and Mohit Bansal</font></i></li>
<li>Learning to Parse Grounded Language using Reservoir Computing<br><i><font size=2>Xavier Hinaut and Michael Spranger</font></i></li>
<li><a href="https://arxiv.org/abs/1811.10092">Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning for Vision-Language Navigation</a><br><i><font size=2>Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang Wang and Lei Zhang</font></i></li>
<li><a href="nonarchival/multirobot_dialog.pdf" target="_blank">A Research Platform for Multi-Robot Dialogue with Humans</a><br><i><font size=2>Matthew Marge, Stephen Nogar, Cory Hayes, Stephanie M. Lukin, Jesse Bloecker, Eric Holder and Clare Voss</font></i></li>
<li>From Spatial Relations to Spatial Configurations<br><i><font size=2>Soham Dan, Parisa Kordjamshidi, Julia Bonn, Archna Bhatia, Martha Palmer and Dan Roth</font></i></li>
<li><a href="https://arxiv.org/abs/1903.02547">Tactical Rewind: Self-Correction via Backtracking in Vision-and-Language Navigation</a><br><i><font size=2>Liyiming Ke, Xiujun Li, Yonatan Bisk, Ari Holtzman, Zhe Gan, Jingjing Liu, Jianfeng Gao, Yejin Choi and Siddhartha Srinivasa</font></i></li>
<li><a href="https://arxiv.org/abs/1904.05521v2" target="_blank">UniVSE: Robust Visual Semantic Embeddings via Structured Semantic Representations</a><br><i><font size=2>Hao Wu, Jiayuan Mao, Yufeng Zhang, Yuning Jiang, Lei Li, Weiwei Sun and Wei-Ying Ma</font></i></li>
</ul>
</font> -->





        <h2 id="overview" class="anchor" aria-hidden="True" href="#overview">Overview and Call For Papers</h2>

        <p>
            Vision and language is a recently raised research area and has received a lot of attention. Initial research
            and applications in this area are mainly image-focused, such as Image Captioning, Visual Question Answering,
            and Referring Expression. However, moving beyond static images is essential for vision and language
            understanding as videos contain much richer information like spatial-temporal dynamics and audio signals. So
            most recently, researchers in both computer vision and natural language processing communities are striving
            to bridge videos and natural language. Popular topics such as video captioning, video question answering,
            text guided video generation fall into this area. We are proposing the first <b>Language &amp; Vision with
                applications to Video Understanding</b> in CVPR with a joint <b>VATEX Video Captioning Challenge</b> and
            a <b>YouMakeup Video Question Answering Challenge</b>. This workshop offers to gather researchers from
            multiple domains to form a new video-language community and attract more people on this topic. In the
            workshop, we will invite several top-tier researchers from this area to present their most recent works. We
            will cover different video-language related topics such as video captioning and video question answering.
            The invited speakers will present key architectural building
            blocks and novel algorithms used to solve these tasks.</p>

        <p>This workshop covers (but is not limited to) the following topics: </p>

        <ul>
            <li> Video captioning, dialogue, and question-answering; </li>
            <li> Sequence learning towards bridging video and language; </li>
            <li> Novel tasks which combine language and video; </li>
            <li> Understanding the relationship between language and video in humans; </li>
            <li> Video synthesis from language; </li>
            <li> Stories as means of abstraction; </li>
            <li> Transfer learning across language and video; </li>
            <li> Joint video and language alignment and parsing; </li>
            <li> Cross-modal learning beyond image understanding, such as videos and audios; </li>
            <li> Multidisciplinary study that may involve linguistics, cognitive science, robotics, etc. </li>
        </ul>

        <p>In addition, We will call for 10-15 high-quality 4 pages extended abstracts to be showcased at a poster
            session along with short talk spotlights. Abstracts are not archival and will not be included in the
            Proceedings of CVPR 2020. In the interests of fostering a freer exchange of ideas we welcome both novel and
            previously-published work. </p>

        <br>
        <a id="submission-info" class="anchor" href="#submission-info" aria-hidden="true"><span
                class="octicon octicon-link"></span></a>
        <!--
<h4>Camera-ready details</h4>
Archival track camera-ready papers should be prepared with NAACL style, either 9 pages without references (long papers) or up to 5 pages without references (short papers). Please make submissions via softconf <a href="https://www.softconf.com/naacl2019/splu">here</a> by <b>April 8</b>.
<br/><br/>
Non-archival track camera-ready papers should be uploaded online (e.g., to arxiv), and links to those camera-ready copies sent to the organizing committee at splu-robonlp-2019@googlegroups.com
<br/><br/>
-->

        <h4>Submission details</h4>
        <p>This track follows the CVPR paper format. Submissions may consist of up to 4 pages of content (excluding
            references) in CVPR format, plus unlimited references. We are also accepting full submissions which will not
            be included in the Proceedings of CVPR 2020 but we will at the option of the authors provide a link to the
            relevant arXiv submission. The submission should be emailed as a single PDF to the <a
                href="mailto:languageandvision@gmail.com">languageandvision@gmail.com</a></p>

        <p>The format of submitted papers to the archival track must follow the CVPR Author Guidelines.
            Style sheets (Latex, Word) are available <a
                href="http://cvpr2020.thecvf.com/submission/main-conference/author-guidelines#submission-guidelines"
                target="_blank">here</a>.





            <!-- <h4><a href="https://www.aclweb.org/adminwiki/index.php?title=Anti-Harassment_Policy">ACL Anti-Harassment Policy</a></h4> -->

            <a id="important-dates" class="anchor" href="#accepted-papers" aria-hidden="true"><span
                    class="octicon octicon-link"></span></a>
        </p>
        <h2>Important Dates</h2>
        <ul>
            <li><b>Submission&nbsp;Deadline: May 6, 2020</b> (11:59pm Anywhere on Earth time, UTC-12)</li>
            <li>Notification: May 12, 2020</li>
            <li>Workshop Day: June 19, 2020</li>
        </ul>


        <a id="challenges" class="anchor" href="#challenges" aria-hidden="true"><span
                class="octicon octicon-link"></span></a>
        <h2>Challenges</h2>
        <h3>VATEX Captioning Challenge 2020</h3>
        <p>This VATEX Captioning Challenge 2020 aims to benchmark progress towards models that can describe the videos
            in various languages such as English and Chinese. This year, in addition to the original 34,991 videos, we
            release a private test set with 6,278 new videos for evaluation.</p>
        <p>Please visit <a href="https://eric-xw.github.io/vatex-website/captioning_2020.html">VATEX Captioning
                Challenge 2020 website</a> for more details!</p>

        <h3>YouMakeup VQA Challenge</h3>
        <p>The YouMakeup VQA challenge aims to provide a common benchmark for fine-grained action understanding in
            domain-specific videos e.g. makeup instructional videos. The makeup instructional videos are naturally more
            fine-grained than open-domain videos. Different action steps contain subtle but critical differences in
            actions, tools and applied facial areas. </p>
        <p>We propose two question-answering tasks to evaluate models' fine-grained action understanding abilities.
            The first task is <b>Facial Image Ordering</b>, which aims to understand visual effects of different actions
            expressed in natural language on facial object.
            The second task is <b>Step Ordering</b>, which aims to measure cross-modal semantic alignments between
            untrimmed long videos and multi-sentence texts.</p>
        <p>Please visit <a href="https://languageandvision.github.io/youmakeup_vqa/index.html">YouMakeup VQA Challenge
                website</a> for more details!</p>


        <a id="organizers" class="anchor" href="#organizers" aria-hidden="true"><span
                class="octicon octicon-link"></span></a>

        <h2>Organizers and PC</h2>
        <div class="container" style="width:900px;margin: auto;">
            <div class="panel panel-default">
                <div class="panel-heading">
                    <h4 class="panel-title">
                        Organizers
                    </h4>
                </div>
                <div id="collapse1">
                    <div class="panel-body">
                        <table cellspacing="0" cellpadding="0" style="width:100%">

                            <tbody>
                                <tr>
                                    <td>
                                        <li><a href="http://www.qi-wu.me/">Qi Wu</a></li>
                                    </td>
                                    <td>University of Adelaide</td>
                                    <td>qi.wu01@adelaide.edu.au</td>
                                </tr>

                                <tr>
                                    <td>
                                        <li><a href="https://eric-xw.github.io/"> Xin Wang</a></li>
                                    </td>
                                    <td>UC Santa Cruz</td>
                                    <td>xwang366@ucsc.edu</td>
                                </tr>

                                <tr>
                                    <td>
                                        <li><a href="https://www.cs.jhu.edu/~cxliu/"> Chenxi Liu</a></li>
                                    </td>
                                    <td>Johns Hopkins University</td>
                                    <td>cxliu@jhu.edu</td>
                                </tr>

                                <tr>
                                    <td>
                                        <li><a href="https://www.cs.unc.edu/~licheng/"> Licheng Yu</a></li>
                                    </td>
                                    <td>Facebook AI</td>
                                    <td>lichengyu@fb.com</td>
                                </tr>

                                <tr>
                                    <td>
                                        <li><a href="https://www.cs.cmu.edu/~lujiang/"> Lu Jiang</a></li>
                                    </td>
                                    <td>Google AI</td>
                                    <td>lujiang@google.com</td>
                                </tr>

                                <tr>
                                    <td>
                                        <li> Yan Huang</li>
                                    </td>
                                    <td>UCAS, China</td>
                                    <td>yhuang@nlpr.ia.ac.cn</td>
                                </tr>

                                <tr>
                                    <td>
                                        <li> Ting Yao</li>
                                    </td>
                                    <td>JD AI Research</td>
                                    <td>tingyao.ustc@gmail.com</td>
                                </tr>

                                <tr>
                                    <td>
                                        <li>Qin Jin</li>
                                    </td>
                                    <td>Renmin University of China</td>
                                    <td>qjin@ruc.edu.cn</td>
                                </tr>

                                <tr>
                                    <td>
                                        <li><a href="https://sites.cs.ucsb.edu/~william/"> William Wang</a></li>
                                    </td>
                                    <td>UC Santa Barbara</td>
                                    <td>william@cs.ucsb.edu</td>
                                </tr>

                                <tr>
                                    <td>
                                        <li><a href="https://cs.adelaide.edu.au/~hengel/"> Anton van den Hengel</a></li>
                                    </td>
                                    <td>University of Adelaide</td>
                                    <td>anton.vandenhengel@adelaide.edu.au</td>
                                </tr>





                            </tbody>
                        </table>

                    </div>
                </div>
                <p style="margin-left: 20px;">Contact the Organizing Committee: <a
                        href="mailto:languageandvision@gmail.com">languageandvision@gmail.com</a></p>
            </div>

            <!-- Contact Organizing Committee:<a href="mailto:splu-robonlp-2019@googlegroups.com?Subject=SpLU-RoboNLP" target="_top"> splu-robonlp-2019@googlegroups.com </a> </p> -->

            <!--   <div class="panel panel-default">
    <div class="panel-heading">
      <h4 class="panel-title">
        Program Committee
      </h4>
    </div>
    <div id="collapse2">
      <div class="panel-body">
        <table cellspacing="0" cellpadding="0" style="width:65%">
	<tr><td><li>Jacob Andreas</td> <td>MIT</td></li> <td> </td></tr>
        <tr><td><li>Angel Chang</td><td>Simon Fraser Univeristy</li></td> <td> </td></tr>
        <tr><td><li>Devendra Chaplot</td><td>CMU</li></td> <td> </td></tr>
        <tr><td><li>Abhishek Das</td><td>Georgia Tech</li></td> <td> </td></tr>
        <tr><td><li>Daniel Fried </td><td>UC Berkeley</li></td> <td> </td></tr>
        <tr><td><li>Zhe Gan</td><td>Microsoft</li></td> <td> </td></tr>
        <tr><td><li>Christopher Kanan</td><td>Rochester Institute of Technology</td></li> <td> </td></tr>
        <tr><td><li>Jiasen Lu</td><td>Georgia Tech</li></td> <td> </td></tr>
        <tr><td><li>Ray Mooney</td><td>University of Texas, Austin </td></li> <td> </td></tr>
        <tr><td><li>Khanh Nguyen</td><td>University of Maryland </td></li> <td> </td></
tr>
        <tr><td><li>Aishwarya Padmakumar</td> <td>University of Texas, Austin </td></li> <td> </td></tr>
	<tr><td><li>Hamid Palangi</td> <td>Microsoft Research</td></li> <td> </td></tr>
        <tr><td><li>Alessandro Suglia</td><td>Heriot-Watt University</td></li> <td> </td></tr>
        <tr><td><li>Shizhe Chen</td><td>Renmin University of China</td></li> <td> </td></tr>
        </table>
      </div>
      <p style="color: #8C1515; margin-left: 20px;">If you are interested in taking a more active part in the workshop, apply to <a href="https://forms.gle/voyxjQLFb8duYM5e7">join the program committee</a>.</p>
    </div>
  </div>
</div>
 -->

            <!-- <a id="pastworkshops" class="anchor" href="#pastworkshops" aria-hidden="true"><span class="octicon octicon-link"></span></a>
<h2>Past Workshops</h2>

<p>
<a href="https://spatial-language.github.io/old_SpLU_workshops/SpLU_2018/"> SpLU 2018</a>
</p>
<p><a href=" https://robo-nlp.github.io/2017_index.html"> Robo-NLP 2017</a>
</p>

<h2>Sponsors</h2>
<p><img src="ms_logo_cam.gif" width="25%"></p>
<p><img src="google_logo.png" width="25%"></p>
</section> -->



        </div>
    </section>
</body>

</html>